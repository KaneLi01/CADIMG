"""
æ¨¡å‹è®¾ç½®å™¨ - è´Ÿè´£æ¨¡å‹åˆå§‹åŒ–ã€Pipelineæ„å»ºå’Œä¼˜åŒ–å™¨è®¾ç½®
"""
from typing import Tuple
import torch
import torch.nn as nn
import torch.optim as optim
from diffusers import StableDiffusionControlNetPipeline

from models.imgedit.diffusion import Diffusion_Models


class ModelSetup:
    """æ¨¡å‹è®¾ç½®å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.device
        self.models = None
        self.train_pipe = None
        self.optimizer = None
        self.loss_fn = None
        
    def initialize_models(self) -> None:
        """åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹"""
        print("Initializing diffusion models...")
        self.models = Diffusion_Models(self.config)

    def create_training_pipeline(self) -> StableDiffusionControlNetPipeline:
        """
        åˆ›å»ºè®­ç»ƒPipeline
        
        Returns:
            è®­ç»ƒç”¨çš„Pipeline
        """
        if self.models is None:
            raise ValueError("Models must be initialized first")
            
        print("Creating training pipeline...")
        
        self.train_pipe = StableDiffusionControlNetPipeline(
            vae=self.models.vae,
            text_encoder=None,  # ä¸ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨ï¼Œè€Œæ˜¯ä½¿ç”¨CLIPå›¾åƒç¼–ç 
            tokenizer=None,
            unet=self.models.unet,
            controlnet=self.models.controlnet,
            scheduler=self.models.scheduler,
            safety_checker=None,
            feature_extractor=None
        ).to(self.device)
        
        # å†»ç»“UNetå‚æ•° - åªè®­ç»ƒControlNet
        self._freeze_unet_parameters()
        
        print("âœ“ Training pipeline created")
        return self.train_pipe
    
    def _freeze_unet_parameters(self) -> None:
        """å†»ç»“UNetå‚æ•°"""
        frozen_params = 0
        total_params = 0
        
        for param in self.train_pipe.unet.parameters():
            total_params += 1
            param.requires_grad = False
            frozen_params += 1
            
        print(f"âœ“ UNet parameters frozen: {frozen_params}/{total_params}")
        
    def setup_optimizer(self) -> optim.Optimizer:
        """
        è®¾ç½®ä¼˜åŒ–å™¨
        
        Returns:
            é…ç½®å¥½çš„ä¼˜åŒ–å™¨
        """
        if self.train_pipe is None:
            raise ValueError("Training pipeline must be created first")
            
        print("Setting up optimizer...")
        
        # åªä¼˜åŒ–ControlNetå‚æ•°
        trainable_params = list(self.train_pipe.controlnet.parameters())
        trainable_count = sum(p.numel() for p in trainable_params if p.requires_grad)
        
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.lr,
            weight_decay=self.config.weight_decay,
        )
        
        print(f"âœ“ Optimizer setup complete. Trainable parameters: {trainable_count:,}")
        return self.optimizer
    
    def setup_loss_function(self) -> nn.Module:
        """
        è®¾ç½®æŸå¤±å‡½æ•°
        
        Returns:
            æŸå¤±å‡½æ•°
        """
        print("Setting up loss function...")
        
        # ä½¿ç”¨MSEæŸå¤±è¿›è¡Œå™ªå£°é¢„æµ‹
        self.loss_fn1 = nn.MSELoss()
        self.loss_fn2 = nn.MSELoss()
        
        return self.loss_fn1, self.loss_fn2
    
    def get_model_info(self) -> dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if self.train_pipe is None:
            return {"error": "Models not initialized"}
            
        info = {}
        
        # ControlNetå‚æ•°ç»Ÿè®¡
        controlnet_params = sum(
            p.numel() for p in self.train_pipe.controlnet.parameters()
        )
        controlnet_trainable = sum(
            p.numel() for p in self.train_pipe.controlnet.parameters() 
            if p.requires_grad
        )
        
        # UNetå‚æ•°ç»Ÿè®¡  
        unet_params = sum(
            p.numel() for p in self.train_pipe.unet.parameters()
        )
        unet_trainable = sum(
            p.numel() for p in self.train_pipe.unet.parameters() 
            if p.requires_grad
        )
        
        info.update({
            'controlnet_params': controlnet_params,
            'controlnet_trainable': controlnet_trainable,
            'unet_params': unet_params,
            'unet_trainable': unet_trainable,
            'total_params': controlnet_params + unet_params,
            'total_trainable': controlnet_trainable + unet_trainable,
            'device': str(self.device),
            'optimizer': type(self.optimizer).__name__ if self.optimizer else None,
            'loss_function': type(self.loss_fn).__name__ if self.loss_fn else None,
        })
        
        return info
    
    def print_model_summary(self) -> None:
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        info = self.get_model_info()
        
        if 'error' in info:
            print(f"âŒ {info['error']}")
            return
            
        print("\n" + "="*50)
        print("ğŸš€ MODEL SUMMARY")
        print("="*50)
        print(f"ğŸ“Š Total Parameters: {info['total_params']:,}")
        print(f"ğŸ¯ Trainable Parameters: {info['total_trainable']:,}")
        print(f"ğŸ“ˆ Training Ratio: {info['total_trainable']/info['total_params']*100:.2f}%")
        print("\nğŸ“‹ Component Details:")
        print(f"  â€¢ ControlNet: {info['controlnet_params']:,} ({info['controlnet_trainable']:,} trainable)")
        print(f"  â€¢ UNet: {info['unet_params']:,} ({info['unet_trainable']:,} trainable)")
        print(f"\nğŸ”§ Configuration:")
        print(f"  â€¢ Device: {info['device']}")
        print(f"  â€¢ Optimizer: {info['optimizer']}")
        print(f"  â€¢ Loss Function: {info['loss_function']}")
        print("="*50 + "\n")
    
    def setup_all(self) -> Tuple[StableDiffusionControlNetPipeline, optim.Optimizer, nn.Module]:
        """
        ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰ç»„ä»¶
        
        Returns:
            (pipeline, optimizer, loss_function)
        """
        print("ğŸš€ Starting model setup...")
        
        # æŒ‰é¡ºåºåˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.initialize_models()
        pipeline = self.create_training_pipeline()
        optimizer = self.setup_optimizer()
        loss_fn = self.setup_loss_function()
        
        # æ‰“å°æ‘˜è¦
        self.print_model_summary()
        
        print("âœ… Model setup complete!\n")
        
        return pipeline, optimizer, loss_fn
    
    def validate_setup(self) -> bool:
        """
        éªŒè¯æ¨¡å‹è®¾ç½®æ˜¯å¦æ­£ç¡®
        
        Returns:
            è®¾ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if self.models is None:
            print("âŒ Models not initialized")
            return False
            
        if self.train_pipe is None:
            print("âŒ Training pipeline not created")
            return False
            
        if self.optimizer is None:
            print("âŒ Optimizer not setup")
            return False
            
        if self.loss_fn is None:
            print("âŒ Loss function not setup")
            return False
            
        # æ£€æŸ¥UNetæ˜¯å¦è¢«æ­£ç¡®å†»ç»“
        unet_trainable = any(p.requires_grad for p in self.train_pipe.unet.parameters())
        if unet_trainable:
            print("âš ï¸  Warning: UNet parameters are not frozen")
            return False
            
        # æ£€æŸ¥ControlNetæ˜¯å¦å¯è®­ç»ƒ
        controlnet_trainable = any(p.requires_grad for p in self.train_pipe.controlnet.parameters())
        if not controlnet_trainable:
            print("âŒ ControlNet parameters are not trainable")
            return False
            
        print("âœ… Model setup validation passed")
        return True


def create_model_setup(config) -> ModelSetup:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ¨¡å‹è®¾ç½®å™¨
    
    Args:
        config: é…ç½®å¯¹è±¡
        
    Returns:
        æ¨¡å‹è®¾ç½®å™¨å®ä¾‹
    """
    return ModelSetup(config)